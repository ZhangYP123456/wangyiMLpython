{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 梯度下降算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "m=100000\n",
    "x=np.random.normal(size=m)\n",
    "X=x.reshape(-1,1)\n",
    "y=x*4.+3.+np.random.normal(0,3,size=m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def J(theta,x_b,y):\n",
    "    try:\n",
    "        return  ((y-x_b.dot(theta))**2)/len(x_b)                         \n",
    "#np.dot(A, B)：对于二维矩阵，计算真正意义上的矩阵乘积，同线性代数中矩阵乘法的定义。对于一维矩阵，计算两者的内积\n",
    "    except:\n",
    "        return float('inf')\n",
    "    \n",
    "def dJ(theta,x_b,y):      #dot函数矩阵乘\n",
    "    res=np.empty(len(theta))  \n",
    "    #empty一样,它所常见的数组内所有元素均为空\n",
    "    res[0]=np.sum(x_b.dot(theta)-y)\n",
    "    for i in range(1,len(theta)):\n",
    "        res[i]=(x_b.dot(theta)-y).dot(x_b[:,1])\n",
    "    return res*2/len(x_b)\n",
    "\n",
    "def gradient_descent(x_b ,y , initial_theta , eta, n_iters=1e4):\n",
    "    theta=initial_theta \n",
    "    i_iter=0\n",
    "    esplion=1e-8\n",
    "    \n",
    "    while i_iter<n_iters:\n",
    "        gradient=dJ(theta,x_b,y)\n",
    "        last_theta=theta\n",
    "        theta=theta-eta * gradient  ###迭代 让theta每次都能向导数的负方向移一步\n",
    "\n",
    "        if(abs((J(last_theta,x_b,y)-J(theta,x_b,y)).any())<esplion):\n",
    "              break\n",
    "        i_iter+=1\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4.84 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "x_b=np.hstack([np.ones((len(x),1)),x.reshape(-1,1)])\n",
    "initial_theta=np.zeros(x_b.shape[1])\n",
    "eta=0.01\n",
    "\n",
    "theta=gradient_descent(x_b, y, initial_theta, eta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.99936539, 3.99912745])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 随机下降算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dJ_sgd(theta,x_b_i,y_i):      #dot函数矩阵乘\n",
    "\n",
    "    return x_b_i.T.dot(x_b_i.dot(theta)-y_i)*2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd(x_b,y,initial_theta,n_iters):\n",
    "    t0=5\n",
    "    t1=50\n",
    "    \n",
    "    def learning_rate(t):\n",
    "        return t0/(t+t1)\n",
    "    theta=initial_theta\n",
    "    for cur_iter in range(n_iters):\n",
    "        rand_i=np.random.randint(int(len(x_b)))\n",
    "        gradient=dJ_sgd(theta,x_b[rand_i],y[rand_i])\n",
    "        theta=theta-learning_rate(cur_iter)*gradient\n",
    "    return theta \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 239 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "x_b=np.hstack([np.ones((len(x),1)),x.reshape(-1,1)])\n",
    "initial_theta=np.zeros(x_b.shape[1])\n",
    "theta=sgd(x_b, y, initial_theta, n_iters=len(x_b)//3)########只检测了三分之一个样本，"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.9413184, 3.9992514])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用自己的随机梯度下降方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "sys.path.append(\"F:/PYCode\")####假如模块搜索的路径\n",
    "from machine_learning.LinearRegression import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "m=100000\n",
    "x=np.random.normal(size=m)\n",
    "X=x.reshape(-1,1)\n",
    "y=x*4.+3.+np.random.normal(0,3,size=m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_r = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin_r.fit_sgd(X, y, n_iters = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.98140876])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin_r.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0032166326681162"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin_r.interception_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 真实数据的使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from machine_learning.model_selection import train_test_split\n",
    "from  sklearn import datasets\n",
    "\n",
    "boston = datasets.load_boston()\n",
    "x= boston.data\n",
    "y = boston.target\n",
    "\n",
    "x1 = x[y<50.0]\n",
    "y1 = y[y<50.0]\n",
    "\n",
    "x_train,y_train,x_test,y_test=train_test_split(x1,y1,seed =666)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "stand =  StandardScaler()\n",
    "stand.fit(x_train)\n",
    "x_train_stand = stand.transform(x_train)\n",
    "x_test_stand = stand.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 11 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5680159406026453"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from machine_learning.LinearRegression import LinearRegression\n",
    "lin_reg10 = LinearRegression()\n",
    "%time lin_reg10.fit_sgd(x_train_stand,y_train,n_iters=2)\n",
    "lin_reg10.score(x_test_stand,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 122 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.813447000504467"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from machine_learning.LinearRegression import LinearRegression\n",
    "lin_reg10 = LinearRegression()\n",
    "%time lin_reg10.fit_sgd(x_train_stand,y_train,n_iters=50)\n",
    "lin_reg10.score(x_test_stand,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 196 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.813017194987496"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from machine_learning.LinearRegression import LinearRegression\n",
    "lin_reg10 = LinearRegression()\n",
    "%time lin_reg10.fit_sgd(x_train_stand,y_train,n_iters=100)\n",
    "lin_reg10.score(x_test_stand,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  scikit-learn中的使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_reg = SGDRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from machine_learning.model_selection import train_test_split\n",
    "from  sklearn import datasets\n",
    "\n",
    "boston = datasets.load_boston()\n",
    "x= boston.data\n",
    "y = boston.target\n",
    "\n",
    "x1 = x[y<50.0]\n",
    "y1 = y[y<50.0]\n",
    "\n",
    "x_train,y_train,x_test,y_test=train_test_split(x1,y1,seed =666)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "stand =  StandardScaler()\n",
    "stand.fit(x_train)\n",
    "x_train_stand = stand.transform(x_train)\n",
    "x_test_stand = stand.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1e+03 µs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDRegressor'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8032705751514354"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time sgd_reg.fit(x_train_stand,y_train)\n",
    "sgd_reg.score(x_test_stand,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
